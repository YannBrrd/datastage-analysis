{# Template for JDBC source to S3 target ETL jobs #}
"""
AWS Glue ETL Job: {{ job_name }}
Pattern: JDBC to S3 ETL (Database Extract)

Migrated from DataStage: {{ original_job_name }}
Generated: {{ generation_timestamp }}
Source Database: {{ source_db_type | default('JDBC') }}
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as F

# Initialize
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'CONNECTION_NAME',
    'DATABASE_NAME',
    'TABLE_NAME',
    'S3_OUTPUT_PATH',
])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

logger = glueContext.get_logger()

# ============================================================================
# CONFIGURATION
# ============================================================================

CONNECTION_NAME = args['CONNECTION_NAME']
DATABASE_NAME = args['DATABASE_NAME']
TABLE_NAME = args['TABLE_NAME']
OUTPUT_PATH = args['S3_OUTPUT_PATH']

# Fully qualified table name
FULL_TABLE_NAME = f"{DATABASE_NAME}.{TABLE_NAME}"

logger.info(f"Extracting: {FULL_TABLE_NAME} -> {OUTPUT_PATH}")

# ============================================================================
# READ FROM DATABASE
# ============================================================================

{% if use_catalog %}
# Option 1: Read from Glue Data Catalog (pre-crawled table)
source_dyf = glueContext.create_dynamic_frame.from_catalog(
    database="{{ catalog_database }}",
    table_name="{{ catalog_table }}",
    transformation_ctx="source_ctx",
    {% if bookmark_keys %}
    additional_options={
        "jobBookmarkKeys": {{ bookmark_keys | tojson }},
        "jobBookmarkKeysSortOrder": "asc"
    }
    {% endif %}
)
{% else %}
# Option 2: Direct JDBC connection
source_dyf = glueContext.create_dynamic_frame.from_options(
    connection_type="jdbc",
    connection_options={
        "useConnectionProperties": "true",
        "connectionName": CONNECTION_NAME,
        "dbtable": FULL_TABLE_NAME,
        {% if parallel_read_column %}
        # Parallel read configuration
        "hashexpression": "{{ parallel_read_column }}",
        "hashpartitions": "{{ parallel_partitions | default(10) }}",
        {% endif %}
        {% if incremental_column %}
        # Incremental read (for large tables)
        "jobBookmarkKeys": ["{{ incremental_column }}"],
        "jobBookmarkKeysSortOrder": "asc",
        {% endif %}
    },
    transformation_ctx="source_ctx"
)
{% endif %}

source_df = source_dyf.toDF()
record_count = source_df.count()
logger.info(f"Read {record_count} records from {FULL_TABLE_NAME}")

# ============================================================================
# TRANSFORMATIONS
# ============================================================================

output_df = source_df

{% if column_mappings %}
# Column mappings from DataStage Transformer
mapped_dyf = ApplyMapping.apply(
    frame=source_dyf,
    mappings=[
        {% for mapping in column_mappings %}
        ("{{ mapping.source_col }}", "{{ mapping.source_type }}", "{{ mapping.target_col }}", "{{ mapping.target_type }}"),
        {% endfor %}
    ],
    transformation_ctx="mapping_ctx"
)
output_df = mapped_dyf.toDF()
{% endif %}

{% if filters %}
# Filters from DataStage Filter stage
{% for filter in filters %}
output_df = output_df.filter(F.expr("{{ filter.sql_condition }}"))
{% endfor %}
{% endif %}

{% if select_columns %}
# Select specific columns
output_df = output_df.select({{ select_columns | tojson }})
{% endif %}

# Add audit columns
output_df = output_df.withColumn("_extracted_at", F.current_timestamp())
output_df = output_df.withColumn("_source_table", F.lit(FULL_TABLE_NAME))

# ============================================================================
# WRITE TO S3
# ============================================================================

output_dyf = DynamicFrame.fromDF(output_df, glueContext, "output_dyf")

glueContext.write_dynamic_frame.from_options(
    frame=output_dyf,
    connection_type="s3",
    connection_options={
        "path": OUTPUT_PATH,
        {% if partition_keys %}
        "partitionKeys": {{ partition_keys | tojson }},
        {% endif %}
    },
    format="{{ output_format | default('parquet') }}",
    format_options={
        "compression": "snappy",
    },
    transformation_ctx="target_ctx"
)

logger.info(f"Wrote {output_df.count()} records to {OUTPUT_PATH}")

# ============================================================================
# COMPLETE
# ============================================================================

job.commit()
logger.info("Job completed successfully")
