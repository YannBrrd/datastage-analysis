{# Template for simple S3 to S3 ETL jobs #}
"""
AWS Glue ETL Job: {{ job_name }}
Pattern: S3 to S3 ETL (Simple File Processing)

Migrated from DataStage: {{ original_job_name }}
Generated: {{ generation_timestamp }}
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as F

# Initialize
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'S3_INPUT_PATH',
    'S3_OUTPUT_PATH',
    'INPUT_FORMAT',
    'OUTPUT_FORMAT',
])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

logger = glueContext.get_logger()

# ============================================================================
# CONFIGURATION
# ============================================================================

INPUT_PATH = args['S3_INPUT_PATH']
OUTPUT_PATH = args['S3_OUTPUT_PATH']
INPUT_FORMAT = args.get('INPUT_FORMAT', '{{ input_format | default("parquet") }}')
OUTPUT_FORMAT = args.get('OUTPUT_FORMAT', '{{ output_format | default("parquet") }}')

logger.info(f"Processing: {INPUT_PATH} -> {OUTPUT_PATH}")

# ============================================================================
# READ SOURCE
# ============================================================================

source_dyf = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={
        "paths": [INPUT_PATH],
        "recurse": True,
        {% if partition_keys %}
        "groupFiles": "inPartition",
        {% endif %}
    },
    format=INPUT_FORMAT,
    {% if input_format == 'csv' %}
    format_options={
        "withHeader": True,
        "separator": "{{ csv_delimiter | default(',') }}",
        "quoteChar": '"',
    },
    {% endif %}
    transformation_ctx="source_ctx"
)

source_df = source_dyf.toDF()
record_count = source_df.count()
logger.info(f"Read {record_count} records from source")

# ============================================================================
# TRANSFORMATIONS
# ============================================================================

{% if column_mappings %}
# Apply column mappings
output_df = source_df
{% for mapping in column_mappings %}
output_df = output_df.withColumnRenamed("{{ mapping.source }}", "{{ mapping.target }}")
{% endfor %}
{% else %}
output_df = source_df
{% endif %}

{% if filters %}
# Apply filters
{% for filter in filters %}
output_df = output_df.filter({{ filter.condition }})
{% endfor %}
{% endif %}

{% if add_metadata %}
# Add metadata columns
output_df = output_df.withColumn("_load_timestamp", F.current_timestamp())
output_df = output_df.withColumn("_source_file", F.input_file_name())
{% endif %}

# ============================================================================
# WRITE TARGET
# ============================================================================

output_dyf = DynamicFrame.fromDF(output_df, glueContext, "output_dyf")

glueContext.write_dynamic_frame.from_options(
    frame=output_dyf,
    connection_type="s3",
    connection_options={
        "path": OUTPUT_PATH,
        {% if partition_keys %}
        "partitionKeys": {{ partition_keys | tojson }},
        {% endif %}
    },
    format=OUTPUT_FORMAT,
    format_options={
        "compression": "{{ compression | default('snappy') }}",
    },
    transformation_ctx="target_ctx"
)

output_count = output_df.count()
logger.info(f"Wrote {output_count} records to target")

# ============================================================================
# COMPLETE
# ============================================================================

job.commit()
logger.info("Job completed successfully")
