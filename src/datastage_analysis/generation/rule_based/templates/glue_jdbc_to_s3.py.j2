"""
AWS Glue Job: {{ job_name }}
Generated by DataStage Migration Analyzer (rule-based)
Pattern: jdbc_to_s3
Generated at: {{ generated_at }}

Description: Reads data from database and writes to S3
Sources: {% for s in analysis.sources %}{{ s.name }}{% if not loop.last %}, {% endif %}{% endfor %}
Targets: {% for t in analysis.targets %}{{ t.name }}{% if not loop.last %}, {% endif %}{% endfor %}
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import *


def main():
    # Get job arguments
    args = getResolvedOptions(sys.argv, [
        "JOB_NAME",
        "jdbc_connection_name",
        "source_database",
        "source_schema",
        "source_table",
        "target_s3_path",
        "target_format",
    ])

    # Initialize Glue context
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args["JOB_NAME"], args)

    try:
        # ========================================
        # Read from Database
        # ========================================
{% for source in analysis.sources %}
        # Source: {{ source.name }} ({{ source.original_type }})
        df_{{ source.name | lower | replace(' ', '_') }} = glueContext.create_dynamic_frame.from_catalog(
            database=args["source_database"],
            table_name=args["source_table"],
            transformation_ctx="source_{{ loop.index }}_ctx",
            additional_options={
                "jobBookmarkKeys": ["id"],  # Adjust to your primary key
                "jobBookmarkKeysSortOrder": "asc"
            }
        ).toDF()

        print(f"Read {{ source.name }}: {df_{{ source.name | lower | replace(' ', '_') }}.count()} rows")
{% endfor %}

        # ========================================
        # Transformations
        # ========================================
{% if analysis.transforms %}
{% for transform in analysis.transforms %}
        # Transform: {{ transform.name }} ({{ transform.original_type }})
        # TODO: Implement transformation logic from {{ transform.name }}
{% endfor %}
{% else %}
        # No transformations - passing through
{% endif %}

        # Main dataframe for output
        df_output = df_{{ analysis.sources[0].name | lower | replace(' ', '_') if analysis.sources else 'input' }}

        # ========================================
        # Write to S3
        # ========================================
{% for target in analysis.targets %}
        # Target: {{ target.name }} ({{ target.original_type }})
        target_format = args.get("target_format", "{{ target.format | default('parquet') }}")

        output_frame = DynamicFrame.fromDF(df_output, glueContext, "output_frame")

        glueContext.write_dynamic_frame.from_options(
            frame=output_frame,
            connection_type="s3",
            connection_options={
                "path": args["target_s3_path"],
                "partitionKeys": [],  # Add partition keys if needed
            },
            format=target_format,
            format_options={
                "compression": "snappy"
            } if target_format == "parquet" else {},
            transformation_ctx="target_{{ loop.index }}_ctx"
        )

        print(f"Wrote {{ target.name }}: {df_output.count()} rows to {args['target_s3_path']}")
{% endfor %}

        job.commit()
        print("Job completed successfully")

    except Exception as e:
        print(f"Job failed with error: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
