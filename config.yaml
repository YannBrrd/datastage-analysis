# DataStage Migration Analyzer Configuration
# ==========================================

# Parser settings
parser:
  # Maximum file size before warning (in MB)
  max_file_size_mb: 510

  # Maximum lines to parse per file (0 = unlimited)
  max_lines: 0

  # Number of parallel workers for parsing
  max_workers: 4

# Migration prediction settings
prediction:
  # Base success probability (0.0 - 1.0)
  success_baseline: 0.85

  # Effort estimation multiplier (adjust based on actual results)
  effort_factor: 1.0

  # Confidence adjustment
  confidence_adjustment: 0.0

# Glue generation settings
glue:
  # Default Glue version
  glue_version: "4.0"

  # Default worker type (Standard, G.1X, G.2X, G.4X, G.8X, Z.2X)
  default_worker_type: "G.1X"

  # Default number of workers
  default_num_workers: 2

  # Enable job bookmarks by default
  enable_bookmarks: true

  # Enable Spark metrics
  enable_metrics: true

# Output settings
output:
  # Default output directory
  output_dir: "./output"

  # Migration wave size (jobs per wave)
  wave_size: 50

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# ===========================================
# LLM Configuration
# ===========================================
llm:
  # Enable/disable LLM features entirely
  enabled: true

  # Provider: anthropic, azure, azure_foundry, aws, gcp, openrouter
  provider: "anthropic"

  # Model selection per provider
  models:
    anthropic: "claude-sonnet-4-20250514"
    azure: "gpt-4o"
    azure_foundry: "meta-llama-3.1-70b-instruct"
    aws: "anthropic.claude-3-sonnet-20240229-v1:0"
    gcp: "gemini-1.5-pro"
    openrouter: "anthropic/claude-sonnet-4"

  # Provider-specific settings
  providers:
    anthropic:
      # API key from environment: ANTHROPIC_API_KEY
      api_key: null  # Set via env var or override here
      base_url: null  # Optional API endpoint override

    azure:
      # Azure OpenAI Service
      # API key from environment: AZURE_OPENAI_API_KEY
      api_key: null
      endpoint: null  # From AZURE_OPENAI_ENDPOINT
      api_version: "2024-02-15-preview"
      deployment_name: "gpt-4o"

    azure_foundry:
      # Azure AI Foundry (formerly Azure AI Studio)
      # API key from environment: AZURE_FOUNDRY_API_KEY
      api_key: null
      endpoint: null  # From AZURE_FOUNDRY_ENDPOINT

    aws:
      # AWS Bedrock - uses standard AWS credential chain
      region: "us-east-1"
      profile: null  # Optional AWS profile name

    gcp:
      # GCP Vertex AI - uses Application Default Credentials
      project_id: null  # From GOOGLE_CLOUD_PROJECT
      location: "us-central1"

    openrouter:
      # OpenRouter - unified API for multiple providers
      # API key from environment: OPENROUTER_API_KEY
      api_key: null
      base_url: "https://openrouter.ai/api/v1"

  # Request settings
  settings:
    temperature: 0.2          # Low for deterministic output
    max_tokens: 4096
    timeout_seconds: 120
    max_retries: 3
    retry_delay_seconds: 2

# ===========================================
# Response Caching
# ===========================================
cache:
  # Enable/disable caching
  enabled: true

  # Cache backend: sqlite, memory
  backend: "sqlite"

  # Cache file location
  path: ".cache/llm_cache.db"

  # Cache TTL in hours (168 = 7 days)
  ttl_hours: 168

# ===========================================
# Code Generation
# ===========================================
generation:
  # Output directory for generated code
  output_dir: "./generated"

  # Target platform: glue, sql
  target: "glue"

  # What to generate
  outputs:
    glue_scripts: true
    terraform: true
    cloudformation: false
    unit_tests: true
    documentation: true

  # Strategy per migration category
  strategy:
    auto:
      use_llm: false
      generator: "rule_based"
    semi_auto:
      use_llm: true
      generator: "hybrid"
    manual:
      use_llm: true
      generator: "llm_based"

  # Validation settings
  validation:
    syntax_check: true
    import_check: true

# ===========================================
# SQL Target Configuration
# ===========================================
sql:
  # SQL dialect: teradata, postgresql, oracle, sqlserver, generic
  dialect: "teradata"

  # Output mode
  generate_stored_procedures: false  # Simple scripts by default
  generate_batch_scripts: true       # BTEQ/TPT for batch processing

  # Infrastructure generation
  generate_ddl: true                 # CREATE TABLE, etc.
  generate_grants: true              # User/role grants
  generate_schemas: true             # CREATE SCHEMA/DATABASE

  # Teradata-specific options
  teradata:
    use_multiset: true          # MULTISET TABLE (vs SET)
    use_primary_index: true     # Generate PRIMARY INDEX
    use_bteq: true              # Use BTEQ for batch scripts
    use_tpt: false              # Use TPT for high-volume loads
    fallback: false             # FALLBACK option
    journal: false              # JOURNAL option
    default_database: "DBC"     # Default database
